#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HuggingFace Hub CLI Tool - ç®€åŒ–æ¨¡å‹å’Œæ•°æ®é›†ç®¡ç†
Author: Kimi Code CLI | Version: 1.0.0 | License: MIT
"""

import os
import sys
import json
import argparse
import shutil
from pathlib import Path
from typing import Optional, List, Dict, Any

try:
    from huggingface_hub import (
        HfApi, HfFolder, snapshot_download, list_models, list_datasets,
        model_info, dataset_info, whoami, login as hf_login, logout as hf_logout
    )
    from huggingface_hub.utils import RepositoryNotFoundError
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…: pip install huggingface_hub>=0.19.0")
    sys.exit(1)

DEFAULT_CACHE = Path.home() / ".cache" / "huggingface"


class HuggingFaceCLI:
    """HuggingFace CLIä¸»ç±»"""
    
    def __init__(self, token: Optional[str] = None, cache_dir: Optional[str] = None):
        self.api = HfApi(token=token)
        self.token = token or self._get_token()
        self.cache_dir = Path(cache_dir) if cache_dir else DEFAULT_CACHE
    
    def _get_token(self) -> Optional[str]:
        try:
            return HfFolder.get_token()
        except Exception:
            return None
    
    def _print(self, icon: str, msg: str):
        print(f"{icon} {msg}")
    
    def _format_size(self, size: int) -> str:
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size < 1024:
                return f"{size:.2f} {unit}"
            size /= 1024
        return f"{size:.2f} PB"
    
    def _get_dir_size(self, path: str) -> int:
        return sum(os.path.getsize(os.path.join(dp, f)) 
                   for dp, dn, fn in os.walk(path) for f in fn)
    
    # ===== Tokenç®¡ç† =====
    
    def login(self, token: str) -> bool:
        try:
            hf_login(token=token)
            self.token, self.api = token, HfApi(token=token)
            user = whoami(token=token)
            self._print("âœ…", f"ç™»å½•æˆåŠŸ! æ¬¢è¿, {user.get('name', 'User')}")
            return True
        except Exception as e:
            self._print("âŒ", f"ç™»å½•å¤±è´¥: {e}")
            return False
    
    def logout(self) -> bool:
        try:
            hf_logout()
            self.token, self.api = None, HfApi()
            self._print("âœ…", "å·²ç™»å‡ºHuggingFace")
            return True
        except Exception as e:
            self._print("âŒ", f"ç™»å‡ºå¤±è´¥: {e}")
            return False
    
    # ===== æœç´¢åŠŸèƒ½ =====
    
    def search_models(self, query: str, limit: int = 10) -> List[Dict]:
        print(f"\n{'='*50}\n  ğŸ” æœç´¢æ¨¡å‹: '{query}'\n{'='*50}\n")
        try:
            models = list(list_models(search=query, limit=limit, fetch_config=False))
            if not models:
                self._print("â„¹ï¸", "æœªæ‰¾åˆ°åŒ¹é…çš„æ¨¡å‹")
                return []
            
            results = []
            print(f"æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹:\n")
            for i, m in enumerate(models, 1):
                info = {"id": m.modelId, "downloads": m.downloads or 0, 
                        "likes": m.likes or 0, "tags": m.tags or [],
                        "task": m.pipeline_tag or "N/A"}
                results.append(info)
                print(f"  {i}. {m.modelId}")
                print(f"     ğŸ“¥ {info['downloads']:,} | â¤ï¸ {info['likes']:,} | ğŸ”§ {info['task']}")
                if info['tags']:
                    print(f"     ğŸ·ï¸ {', '.join(info['tags'][:5])}")
                print()
            return results
        except Exception as e:
            self._print("âŒ", f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_datasets(self, query: str, limit: int = 10) -> List[Dict]:
        print(f"\n{'='*50}\n  ğŸ” æœç´¢æ•°æ®é›†: '{query}'\n{'='*50}\n")
        try:
            datasets = list(list_datasets(search=query, limit=limit))
            if not datasets:
                self._print("â„¹ï¸", "æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®é›†")
                return []
            
            results = []
            print(f"æ‰¾åˆ° {len(datasets)} ä¸ªæ•°æ®é›†:\n")
            for i, d in enumerate(datasets, 1):
                info = {"id": d.id, "downloads": d.downloads or 0, "tags": d.tags or []}
                results.append(info)
                print(f"  {i}. {d.id}")
                print(f"     ğŸ“¥ {info['downloads']:,}")
                if info['tags']:
                    print(f"     ğŸ·ï¸ {', '.join(info['tags'][:5])}")
                print()
            return results
        except Exception as e:
            self._print("âŒ", f"æœç´¢å¤±è´¥: {e}")
            return []
    
    # ===== ä¿¡æ¯æŸ¥è¯¢ =====
    
    def get_model_info(self, model_id: str) -> Optional[Dict]:
        print(f"\n{'='*50}\n  ğŸ“‹ æ¨¡å‹ä¿¡æ¯: {model_id}\n{'='*50}\n")
        try:
            info = model_info(model_id, token=self.token)
            files = [f.rfilename for f in info.siblings] if info.siblings else []
            result = {"id": info.id, "sha": info.sha[:16] if info.sha else "N/A",
                      "downloads": info.downloads, "likes": info.likes,
                      "task": info.pipeline_tag, "tags": info.tags or [],
                      "files": files, "created": str(info.created_at)[:10],
                      "modified": str(info.last_modified)[:10]}
            
            print(f"  ğŸ†” {result['id']}")
            print(f"  ğŸ”¢ {result['sha']}...")
            print(f"  ğŸ“¥ {result['downloads']:,} | â¤ï¸ {result['likes']:,}")
            print(f"  ğŸ”§ {result['task'] or 'N/A'}")
            print(f"  ğŸ“… {result['created']} | ğŸ“ {result['modified']}")
            print(f"\n  ğŸ·ï¸ æ ‡ç­¾: {', '.join(result['tags'][:8]) if result['tags'] else 'N/A'}")
            print(f"\n  ğŸ“ æ–‡ä»¶ ({len(files)} ä¸ª):")
            for f in files[:15]:
                print(f"     - {f}")
            if len(files) > 15:
                print(f"     ... è¿˜æœ‰ {len(files)-15} ä¸ª")
            return result
        except RepositoryNotFoundError:
            self._print("âŒ", f"æ¨¡å‹ä¸å­˜åœ¨: {model_id}")
            return None
        except Exception as e:
            self._print("âŒ", f"è·å–å¤±è´¥: {e}")
            return None
    
    def get_dataset_info(self, dataset_id: str) -> Optional[Dict]:
        print(f"\n{'='*50}\n  ğŸ“‹ æ•°æ®é›†ä¿¡æ¯: {dataset_id}\n{'='*50}\n")
        try:
            info = dataset_info(dataset_id, token=self.token)
            files = [f.rfilename for f in info.siblings] if info.siblings else []
            result = {"id": info.id, "sha": info.sha[:16] if info.sha else "N/A",
                      "downloads": info.downloads, "tags": info.tags or [],
                      "files": files, "modified": str(info.last_modified)[:10]}
            
            print(f"  ğŸ†” {result['id']}")
            print(f"  ğŸ”¢ {result['sha']}...")
            print(f"  ğŸ“¥ {result['downloads']:,}")
            print(f"  ğŸ“ {result['modified']}")
            print(f"\n  ğŸ·ï¸ æ ‡ç­¾: {', '.join(result['tags'][:8]) if result['tags'] else 'N/A'}")
            print(f"\n  ğŸ“ æ–‡ä»¶ ({len(files)} ä¸ª):")
            for f in files[:15]:
                print(f"     - {f}")
            if len(files) > 15:
                print(f"     ... è¿˜æœ‰ {len(files)-15} ä¸ª")
            return result
        except RepositoryNotFoundError:
            self._print("âŒ", f"æ•°æ®é›†ä¸å­˜åœ¨: {dataset_id}")
            return None
        except Exception as e:
            self._print("âŒ", f"è·å–å¤±è´¥: {e}")
            return None
    
    # ===== ä¸‹è½½åŠŸèƒ½ =====
    
    def download_model(self, model_id: str, local_dir: Optional[str] = None,
                       include: Optional[List[str]] = None, exclude: Optional[List[str]] = None,
                       resume: bool = True) -> Optional[str]:
        print(f"\n{'='*50}\n  â¬‡ï¸  ä¸‹è½½æ¨¡å‹: {model_id}\n{'='*50}\n")
        target = Path(local_dir) if local_dir else self.cache_dir / "hub" / model_id.replace("/", "--")
        
        try:
            self._print("â„¹ï¸", f"ç›®æ ‡: {target}")
            path = snapshot_download(repo_id=model_id, repo_type="model",
                                     local_dir=str(target) if local_dir else None,
                                     cache_dir=self.cache_dir if not local_dir else None,
                                     allow_patterns=include, ignore_patterns=exclude,
                                     resume_download=resume, token=self.token)
            self._print("âœ…", "ä¸‹è½½å®Œæˆ!")
            self._print("â„¹ï¸", f"ä½ç½®: {path}")
            self._print("â„¹ï¸", f"å¤§å°: {self._format_size(self._get_dir_size(path))}")
            return path
        except RepositoryNotFoundError:
            self._print("âŒ", f"æ¨¡å‹ä¸å­˜åœ¨: {model_id}")
            return None
        except Exception as e:
            self._print("âŒ", f"ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def download_dataset(self, dataset_id: str, local_dir: Optional[str] = None,
                         include: Optional[List[str]] = None, exclude: Optional[List[str]] = None,
                         resume: bool = True) -> Optional[str]:
        print(f"\n{'='*50}\n  â¬‡ï¸  ä¸‹è½½æ•°æ®é›†: {dataset_id}\n{'='*50}\n")
        target = Path(local_dir) if local_dir else self.cache_dir / "datasets" / dataset_id.replace("/", "--")
        
        try:
            self._print("â„¹ï¸", f"ç›®æ ‡: {target}")
            path = snapshot_download(repo_id=dataset_id, repo_type="dataset",
                                     local_dir=str(target) if local_dir else None,
                                     cache_dir=self.cache_dir if not local_dir else None,
                                     allow_patterns=include, ignore_patterns=exclude,
                                     resume_download=resume, token=self.token)
            self._print("âœ…", "ä¸‹è½½å®Œæˆ!")
            self._print("â„¹ï¸", f"ä½ç½®: {path}")
            self._print("â„¹ï¸", f"å¤§å°: {self._format_size(self._get_dir_size(path))}")
            return path
        except RepositoryNotFoundError:
            self._print("âŒ", f"æ•°æ®é›†ä¸å­˜åœ¨: {dataset_id}")
            return None
        except Exception as e:
            self._print("âŒ", f"ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    # ===== ç¼“å­˜ç®¡ç† =====
    
    def cache_info(self) -> Dict[str, Any]:
        print(f"\n{'='*50}\n  ğŸ’¾ ç¼“å­˜ä¿¡æ¯\n{'='*50}\n")
        hub_dir = self.cache_dir / "hub"
        ds_dir = self.cache_dir / "datasets"
        
        info = {"cache_dir": str(self.cache_dir), "hub_dir": str(hub_dir),
                "ds_dir": str(ds_dir), "token_exists": (Path.home() / ".huggingface" / "token").exists()}
        
        print(f"  ğŸ“ æ ¹ç›®å½•: {info['cache_dir']}")
        print(f"  ğŸ¤– æ¨¡å‹: {info['hub_dir']}")
        print(f"  ğŸ“Š æ•°æ®é›†: {info['ds_dir']}")
        print(f"  ğŸ”‘ Token: {'å­˜åœ¨' if info['token_exists'] else 'ä¸å­˜åœ¨'}")
        
        hub_size = self._get_dir_size(str(hub_dir)) if hub_dir.exists() else 0
        ds_size = self._get_dir_size(str(ds_dir)) if ds_dir.exists() else 0
        info.update({"hub_size": hub_size, "ds_size": ds_size, "total": hub_size + ds_size})
        
        print(f"\n  ğŸ’½ ç©ºé—´ä½¿ç”¨:")
        print(f"     æ¨¡å‹: {self._format_size(hub_size)}")
        print(f"     æ•°æ®é›†: {self._format_size(ds_size)}")
        print(f"     æ€»è®¡: {self._format_size(info['total'])}")
        return info
    
    def clean_cache(self, force: bool = False) -> bool:
        print(f"\n{'='*50}\n  ğŸ§¹ æ¸…ç†ç¼“å­˜\n{'='*50}\n")
        if not force:
            self._print("âš ï¸", "è¯·ä½¿ç”¨ --force ç¡®è®¤æ¸…ç†")
            return False
        try:
            if self.cache_dir.exists():
                shutil.rmtree(self.cache_dir)
            self._print("âœ…", "ç¼“å­˜å·²æ¸…ç†")
            return True
        except Exception as e:
            self._print("âŒ", f"æ¸…ç†å¤±è´¥: {e}")
            return False
    
    def list_local(self) -> List[str]:
        print(f"\n{'='*50}\n  ğŸ“š æœ¬åœ°æ¨¡å‹\n{'='*50}\n")
        hub_dir = self.cache_dir / "hub"
        if not hub_dir.exists():
            self._print("â„¹ï¸", "æœ¬åœ°æ²¡æœ‰æ¨¡å‹")
            return []
        
        models = [item.name.replace("--", "/") for item in hub_dir.iterdir() if item.is_dir()]
        if models:
            print(f"å…± {len(models)} ä¸ªæ¨¡å‹:\n")
            for i, m in enumerate(sorted(models), 1):
                size = self._get_dir_size(str(hub_dir / m.replace("/", "--")))
                print(f"  {i}. {m} ({self._format_size(size)})")
        else:
            self._print("â„¹ï¸", "æœ¬åœ°æ²¡æœ‰æ¨¡å‹")
        return models


def main():
    parser = argparse.ArgumentParser(description="HuggingFace Hub CLI", 
                                     formatter_class=argparse.RawDescriptionHelpFormatter,
                                     epilog="""ç¤ºä¾‹:
  python main.py search -q bert-base -l 5
  python main.py download -m bert-base-chinese --local-dir ./models
  python main.py info -m bert-base-chinese
  python main.py login -t your_token""")
    
    parser.add_argument("action", choices=["search", "download", "info", "login", "logout", 
                        "cache", "list", "dataset-search", "dataset-download", "dataset-info"],
                        help="æ“ä½œç±»å‹")
    parser.add_argument("-q", "--query", help="æœç´¢å…³é”®è¯")
    parser.add_argument("-l", "--limit", type=int, default=10, help="ç»“æœé™åˆ¶")
    parser.add_argument("-m", "--model", help="æ¨¡å‹ID")
    parser.add_argument("-d", "--dataset", help="æ•°æ®é›†ID")
    parser.add_argument("-t", "--token", help="è®¿é—®ä»¤ç‰Œ")
    parser.add_argument("--local-dir", help="æœ¬åœ°ç›®å½•")
    parser.add_argument("--cache-dir", help="ç¼“å­˜ç›®å½•")
    parser.add_argument("--include", nargs="+", help="åŒ…å«æ¨¡å¼")
    parser.add_argument("--exclude", nargs="+", help="æ’é™¤æ¨¡å¼")
    parser.add_argument("--resume", action="store_true", default=True, help="æ–­ç‚¹ç»­ä¼ ")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶æ“ä½œ")
    
    args = parser.parse_args()
    cli = HuggingFaceCLI(token=args.token, cache_dir=args.cache_dir)
    
    if args.action == "search":
        if not args.query:
            print("âŒ è¯·æä¾›: --query <å…³é”®è¯>"); sys.exit(1)
        cli.search_models(args.query, args.limit)
    elif args.action == "dataset-search":
        if not args.query:
            print("âŒ è¯·æä¾›: --query <å…³é”®è¯>"); sys.exit(1)
        cli.search_datasets(args.query, args.limit)
    elif args.action == "download":
        if not args.model:
            print("âŒ è¯·æä¾›: --model <æ¨¡å‹ID>"); sys.exit(1)
        cli.download_model(args.model, args.local_dir, args.include, args.exclude, args.resume)
    elif args.action == "dataset-download":
        if not args.dataset:
            print("âŒ è¯·æä¾›: --dataset <æ•°æ®é›†ID>"); sys.exit(1)
        cli.download_dataset(args.dataset, args.local_dir, args.include, args.exclude, args.resume)
    elif args.action == "info":
        if not args.model:
            print("âŒ è¯·æä¾›: --model <æ¨¡å‹ID>"); sys.exit(1)
        cli.get_model_info(args.model)
    elif args.action == "dataset-info":
        if not args.dataset:
            print("âŒ è¯·æä¾›: --dataset <æ•°æ®é›†ID>"); sys.exit(1)
        cli.get_dataset_info(args.dataset)
    elif args.action == "login":
        if not args.token:
            print("âŒ è¯·æä¾›: --token <token>"); sys.exit(1)
        cli.login(args.token)
    elif args.action == "logout":
        cli.logout()
    elif args.action == "cache":
        cli.cache_info()
    elif args.action == "list":
        cli.list_local()


if __name__ == "__main__":
    main()
