#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EstiToken - Tokenæ¶ˆè€—ä¼°ç®—å™¨
æ™ºèƒ½ä»»åŠ¡Tokenæ¶ˆè€—ä¼°ç®—ä¸æˆæœ¬åˆ†æå·¥å…·

å€Ÿé‰´: tiktoken, OpenAI Tokenè®¡æ•°å™¨, Claude Code Tokenä¼˜åŒ–ç­–ç•¥
"""

import argparse
import json
import os
import re
import sys
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import fnmatch


@dataclass
class ModelPricing:
    """æ¨¡å‹å®šä»·ä¿¡æ¯"""
    name: str
    input_price: float  # æ¯1K tokensä»·æ ¼(USD)
    output_price: float
    max_context: int
    description: str


# æ¨¡å‹å®šä»·è¡¨ (USD per 1K tokens)
MODELS = {
    "kimi-for-coding": ModelPricing(
        name="kimi-for-coding",
        input_price=0.005,
        output_price=0.025,
        max_context=262144,
        description="Kimiæ ‡å‡†æ¨¡å‹ï¼Œé€‚åˆæ—¥å¸¸ç¼–ç ä»»åŠ¡"
    ),
    "kimi-k2": ModelPricing(
        name="kimi-k2-0905-preview",
        input_price=0.015,
        output_price=0.060,
        max_context=262144,
        description="Kimié«˜çº§æ¨¡å‹ï¼Œé€‚åˆå¤æ‚æ¨ç†"
    ),
    "gpt-4": ModelPricing(
        name="gpt-4",
        input_price=0.030,
        output_price=0.060,
        max_context=8192,
        description="GPT-4æ ‡å‡†ç‰ˆ"
    ),
    "gpt-4-turbo": ModelPricing(
        name="gpt-4-turbo",
        input_price=0.010,
        output_price=0.030,
        max_context=128000,
        description="GPT-4 Turboï¼Œæ›´å¤§ä¸Šä¸‹æ–‡"
    ),
    "claude-3-opus": ModelPricing(
        name="claude-3-opus",
        input_price=0.015,
        output_price=0.075,
        max_context=200000,
        description="Claude 3 Opusï¼Œæœ€å¼ºæ¨ç†"
    ),
    "claude-3-sonnet": ModelPricing(
        name="claude-3-sonnet",
        input_price=0.003,
        output_price=0.015,
        max_context=200000,
        description="Claude 3 Sonnetï¼Œå¹³è¡¡æ€§ä»·æ¯”"
    ),
}

# ä»»åŠ¡å¤æ‚åº¦ç³»æ•°
COMPLEXITY_MULTIPLIERS = {
    "low": 1.0,
    "medium": 2.5,
    "high": 5.0,
    "expert": 8.0,
}

# ä»»åŠ¡ç±»å‹åŸºå‡†Tokenæ¶ˆè€— (è¾“å…¥:è¾“å‡ºæ¯”ä¾‹)
TASK_PATTERNS = {
    "file_read": (100, 50),
    "code_review": (1000, 500),
    "documentation": (500, 1000),
    "refactoring": (1000, 800),
    "debugging": (2000, 1500),
    "architecture": (3000, 2500),
    "planning": (2000, 2000),
    "code_generation": (500, 1500),
    "testing": (800, 600),
    "optimization": (1500, 1000),
}


class TokenEstimator:
    """Tokenä¼°ç®—å™¨æ ¸å¿ƒç±»"""
    
    def __init__(self, stats_path: Optional[Path] = None):
        self.stats_path = stats_path or (Path.home() / ".kimi" / "estitoken-stats.json")
        self.stats = self._load_stats()
    
    def _load_stats(self) -> dict:
        """åŠ è½½ç»Ÿè®¡å†å²"""
        if self.stats_path.exists():
            try:
                with open(self.stats_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass
        return {
            "estimates": [],
            "created_at": datetime.now().isoformat(),
            "version": "1.0.0"
        }
    
    def _save_stats(self):
        """ä¿å­˜ç»Ÿè®¡å†å²"""
        self.stats_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.stats_path, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2, ensure_ascii=False)
    
    def estimate_text(self, text: str, content_type: str = "mixed") -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„Tokenæ•°é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            content_type: å†…å®¹ç±»å‹ (chinese/english/code/mixed)
        
        Returns:
            ä¼°ç®—çš„Tokenæ•°é‡
        """
        if not text:
            return 0
        
        char_count = len(text)
        
        # æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©ç³»æ•°
        if content_type == "chinese":
            # ä¸­æ–‡çº¦ 1.5 å­—ç¬¦/token
            return int(char_count / 1.5)
        elif content_type == "english":
            # è‹±æ–‡çº¦ 4 å­—ç¬¦/token
            return int(char_count / 4)
        elif content_type == "code":
            # ä»£ç çº¦ 3.5 å­—ç¬¦/token
            return int(char_count / 3.5)
        else:  # mixed - æ™ºèƒ½æ£€æµ‹
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            if chinese_chars > english_chars:
                return int(char_count / 2)
            else:
                return int(char_count / 3.5)
    
    def detect_content_type(self, text: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹å†…å®¹ç±»å‹"""
        chinese_ratio = len(re.findall(r'[\u4e00-\u9fff]', text)) / max(len(text), 1)
        code_patterns = len(re.findall(r'[{};()=<>]|def |class |import |function', text))
        
        if code_patterns > 5:
            return "code"
        elif chinese_ratio > 0.3:
            return "chinese"
        else:
            return "english"
    
    def estimate_file(self, filepath: Union[str, Path], 
                     content_type: Optional[str] = None) -> Dict:
        """
        ä¼°ç®—å•ä¸ªæ–‡ä»¶çš„Tokenæ•°é‡
        
        Returns:
            åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸
        """
        path = Path(filepath)
        
        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except Exception as e:
            return {
                "path": str(path),
                "error": str(e),
                "tokens": 0
            }
        
        # è‡ªåŠ¨æ£€æµ‹å†…å®¹ç±»å‹
        detected_type = content_type or self.detect_content_type(content)
        tokens = self.estimate_text(content, detected_type)
        
        return {
            "path": str(path),
            "name": path.name,
            "size": path.stat().st_size,
            "chars": len(content),
            "lines": content.count('\n') + 1,
            "tokens": tokens,
            "content_type": detected_type
        }
    
    def estimate_directory(self, dirpath: Union[str, Path], 
                          pattern: str = "*",
                          recursive: bool = True) -> List[Dict]:
        """
        æ‰¹é‡ä¼°ç®—ç›®å½•ä¸‹çš„æ–‡ä»¶
        
        Args:
            dirpath: ç›®å½•è·¯å¾„
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼ (å¦‚ "*.py", "*.md")
            recursive: æ˜¯å¦é€’å½’å­ç›®å½•
        
        Returns:
            æ–‡ä»¶ä¼°ç®—ç»“æœåˆ—è¡¨
        """
        path = Path(dirpath)
        results = []
        
        if recursive:
            files = list(path.rglob(pattern))
        else:
            files = list(path.glob(pattern))
        
        # æ’é™¤å¸¸è§éæ–‡æœ¬æ–‡ä»¶
        exclude_patterns = ['*.png', '*.jpg', '*.jpeg', '*.gif', '*.ico', 
                          '*.pdf', '*.zip', '*.tar.gz', '*.exe', '*.dll']
        
        for filepath in files:
            if filepath.is_file():
                # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨
                should_exclude = any(fnmatch.fnmatch(filepath.name, p) 
                                   for p in exclude_patterns)
                if should_exclude:
                    continue
                
                try:
                    result = self.estimate_file(filepath)
                    results.append(result)
                except Exception:
                    pass  # è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶
        
        return sorted(results, key=lambda x: x.get('tokens', 0), reverse=True)
    
    def estimate_task(self, task_description: str, 
                     complexity: str = "medium",
                     context_tokens: int = 0) -> Dict:
        """
        åŸºäºä»»åŠ¡æè¿°ä¼°ç®—Tokenæ¶ˆè€—
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            complexity: å¤æ‚åº¦ (low/medium/high/expert)
            context_tokens: å·²æœ‰ä¸Šä¸‹æ–‡Tokenæ•°
        
        Returns:
            ä»»åŠ¡ä¼°ç®—ç»“æœ
        """
        # æ£€æµ‹ä»»åŠ¡ç±»å‹
        task_type = self._detect_task_type(task_description)
        
        # è·å–åŸºå‡†æ¶ˆè€—
        base_input, base_output = TASK_PATTERNS.get(task_type, (1000, 500))
        
        # åº”ç”¨å¤æ‚åº¦ç³»æ•°
        multiplier = COMPLEXITY_MULTIPLIERS.get(complexity, 2.5)
        
        estimated_input = int((base_input + context_tokens) * multiplier)
        estimated_output = int(base_output * multiplier)
        
        return {
            "task": task_description,
            "task_type": task_type,
            "complexity": complexity,
            "multiplier": multiplier,
            "input_tokens": estimated_input,
            "output_tokens": estimated_output,
            "total_tokens": estimated_input + estimated_output,
            "context_tokens": context_tokens
        }
    
    def _detect_task_type(self, description: str) -> str:
        """ä»æè¿°ä¸­æ£€æµ‹ä»»åŠ¡ç±»å‹"""
        description_lower = description.lower()
        
        keywords = {
            "file_read": ["è¯»å–", "æŸ¥çœ‹", "æ‰“å¼€", "read", "open"],
            "code_review": ["å®¡æŸ¥", "review", "æ£€æŸ¥", "ä»£ç å®¡æŸ¥", "codereview"],
            "documentation": ["æ–‡æ¡£", "documentation", "readme", "æ³¨é‡Š"],
            "refactoring": ["é‡æ„", "refactor", "é‡å†™", "ä¼˜åŒ–ä»£ç "],
            "debugging": ["è°ƒè¯•", "debug", "ä¿®å¤", "fix", "bug"],
            "architecture": ["æ¶æ„", "architecture", "è®¾è®¡", "design", "ç³»ç»Ÿ"],
            "planning": ["è§„åˆ’", "è®¡åˆ’", "plan", "roadmap", "é‡Œç¨‹ç¢‘"],
            "code_generation": ["ç”Ÿæˆä»£ç ", "ç¼–å†™", "create", "generate", "å®ç°"],
            "testing": ["æµ‹è¯•", "test", "å•å…ƒæµ‹è¯•", "unittest"],
            "optimization": ["ä¼˜åŒ–", "optimize", "æ€§èƒ½", "æé€Ÿ"],
        }
        
        for task_type, words in keywords.items():
            if any(word in description_lower for word in words):
                return task_type
        
        return "code_generation"  # é»˜è®¤ç±»å‹
    
    def log_estimate(self, result: Dict):
        """è®°å½•ä¼°ç®—å†å²"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            **result
        }
        self.stats["estimates"].append(entry)
        self._save_stats()


class CostAnalyzer:
    """æˆæœ¬åˆ†æå™¨"""
    
    def __init__(self, model: str = "kimi-for-coding"):
        self.model = model
        self.pricing = MODELS.get(model)
        if not self.pricing:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model}")
    
    def calculate_cost(self, input_tokens: int, output_tokens: int = 0) -> Dict:
        """
        è®¡ç®—æˆæœ¬
        
        Returns:
            æˆæœ¬æ˜ç»†å­—å…¸
        """
        input_cost = (input_tokens / 1000) * self.pricing.input_price
        output_cost = (output_tokens / 1000) * self.pricing.output_price
        total_cost = input_cost + output_cost
        
        return {
            "model": self.model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost,
            "currency": "USD"
        }
    
    def compare_models(self, input_tokens: int, 
                       output_tokens: int = 0) -> List[Dict]:
        """å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æˆæœ¬"""
        results = []
        for model_name, pricing in MODELS.items():
            analyzer = CostAnalyzer(model_name)
            cost = analyzer.calculate_cost(input_tokens, output_tokens)
            results.append(cost)
        
        return sorted(results, key=lambda x: x["total_cost"])
    
    def generate_report(self, estimates: List[Dict]) -> str:
        """ç”Ÿæˆæ ¼å¼åŒ–çš„æŠ¥å‘Š"""
        if not estimates:
            return "æ²¡æœ‰ä¼°ç®—æ•°æ®"
        
        total_tokens = sum(e.get('tokens', 0) for e in estimates)
        total_chars = sum(e.get('chars', 0) for e in estimates)
        total_lines = sum(e.get('lines', 0) for e in estimates)
        
        # æˆæœ¬è®¡ç®—
        cost = self.calculate_cost(total_tokens, total_tokens // 2)
        
        lines = [
            "=" * 60,
            "                    Token ä¼°ç®—æŠ¥å‘Š",
            "=" * 60,
            "",
            f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:",
            f"  æ–‡ä»¶æ•°: {len(estimates)}",
            f"  æ€»å­—ç¬¦: {total_chars:,}",
            f"  æ€»è¡Œæ•°: {total_lines:,}",
            f"  æ€»Token: {total_tokens:,}",
            "",
            f"ğŸ’° æˆæœ¬ä¼°ç®— ({self.model}):",
            f"  è¾“å…¥: ${cost['input_cost']:.4f} ({cost['input_tokens']:,} tokens)",
            f"  è¾“å‡º: ${cost['output_cost']:.4f} ({cost['output_tokens']:,} tokens)",
            f"  æ€»è®¡: ${cost['total_cost']:.4f}",
            "",
        ]
        
        # æ·»åŠ æ–‡ä»¶åˆ—è¡¨
        if len(estimates) <= 10:
            lines.append("ğŸ“ æ–‡ä»¶æ˜ç»†:")
            for i, e in enumerate(estimates[:10], 1):
                lines.append(f"  {i}. {e.get('name', 'unknown'):30} {e.get('tokens', 0):>6} tokens")
        else:
            lines.append(f"ğŸ“ Top 10 æ–‡ä»¶ (å…± {len(estimates)} ä¸ª):")
            for i, e in enumerate(estimates[:10], 1):
                lines.append(f"  {i}. {e.get('name', 'unknown'):30} {e.get('tokens', 0):>6} tokens")
        
        lines.append("=" * 60)
        
        return "\n".join(lines)


def print_banner():
    """æ‰“å°æ¨ªå¹…"""
    print("=" * 60)
    print("  EstiToken - Tokenæ¶ˆè€—ä¼°ç®—å™¨ v1.0.0")
    print("  æ™ºèƒ½ä»»åŠ¡Tokenæ¶ˆè€—ä¼°ç®—ä¸æˆæœ¬åˆ†æ")
    print("=" * 60)
    print()


def main():
    parser = argparse.ArgumentParser(
        description="EstiToken - Tokenæ¶ˆè€—ä¼°ç®—å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  estitoken "è¿™æ˜¯ä¸€æ®µä¸­æ–‡æ–‡æœ¬"                    # ä¼°ç®—æ–‡æœ¬
  estitoken --file README.md                       # ä¼°ç®—æ–‡ä»¶
  estitoken --dir ./src --pattern "*.py"          # ä¼°ç®—ç›®å½•
  estitoken --task "é‡æ„é¡¹ç›®" --complexity high   # ä¼°ç®—ä»»åŠ¡
  estitoken --report                               # æŸ¥çœ‹æŠ¥å‘Š
        """
    )
    
    # è¾“å…¥é€‰é¡¹
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument("text", nargs="?", help="è¦ä¼°ç®—çš„æ–‡æœ¬")
    input_group.add_argument("--file", "-f", help="è¦ä¼°ç®—çš„æ–‡ä»¶è·¯å¾„")
    input_group.add_argument("--dir", "-d", help="è¦ä¼°ç®—çš„ç›®å½•è·¯å¾„")
    input_group.add_argument("--task", "-t", help="ä»»åŠ¡æè¿°")
    input_group.add_argument("--report", "-r", action="store_true", help="æ˜¾ç¤ºç»Ÿè®¡æŠ¥å‘Š")
    
    # é€‰é¡¹
    parser.add_argument("--pattern", "-p", default="*", 
                       help="æ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: *)")
    parser.add_argument("--model", "-m", default="kimi-for-coding",
                       choices=list(MODELS.keys()),
                       help="ä½¿ç”¨çš„æ¨¡å‹ (é»˜è®¤: kimi-for-coding)")
    parser.add_argument("--complexity", "-c", default="medium",
                       choices=["low", "medium", "high", "expert"],
                       help="ä»»åŠ¡å¤æ‚åº¦ (é»˜è®¤: medium)")
    parser.add_argument("--compare", action="store_true",
                       help="å¯¹æ¯”æ‰€æœ‰æ¨¡å‹æˆæœ¬")
    parser.add_argument("--json", "-j", action="store_true",
                       help="è¾“å‡ºJSONæ ¼å¼")
    parser.add_argument("--no-banner", action="store_true",
                       help="ä¸æ˜¾ç¤ºæ¨ªå¹…")
    
    args = parser.parse_args()
    
    if not args.no_banner:
        print_banner()
    
    estimator = TokenEstimator()
    analyzer = CostAnalyzer(args.model)
    
    # æ–‡æœ¬ä¼°ç®—
    if args.text:
        content_type = estimator.detect_content_type(args.text)
        tokens = estimator.estimate_text(args.text, content_type)
        cost = analyzer.calculate_cost(tokens, tokens // 2)
        
        result = {
            "type": "text",
            "content_type": content_type,
            "chars": len(args.text),
            "tokens": tokens,
            "cost": cost
        }
        
        if args.json:
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            print(f"æ–‡æœ¬ç±»å‹: {content_type}")
            print(f"å­—ç¬¦æ•°: {len(args.text)}")
            print(f"ä¼°ç®—Token: ~{tokens}")
            print(f"é¢„ä¼°æˆæœ¬: ${cost['total_cost']:.4f}")
    
    # æ–‡ä»¶ä¼°ç®—
    elif args.file:
        try:
            result = estimator.estimate_file(args.file)
            cost = analyzer.calculate_cost(result['tokens'], result['tokens'] // 2)
            result['cost'] = cost
            
            if args.json:
                print(json.dumps(result, indent=2, ensure_ascii=False))
            else:
                print(f"æ–‡ä»¶: {result['name']}")
                print(f"å¤§å°: {result['size']:,} bytes")
                print(f"è¡Œæ•°: {result['lines']}")
                print(f"å­—ç¬¦: {result['chars']}")
                print(f"Token: ~{result['tokens']}")
                print(f"é¢„ä¼°æˆæœ¬: ${cost['total_cost']:.4f}")
                
                if args.compare:
                    print("\næ¨¡å‹å¯¹æ¯”:")
                    comparisons = analyzer.compare_models(result['tokens'], result['tokens'] // 2)
                    for c in comparisons:
                        print(f"  {c['model']:20} ${c['total_cost']:.4f}")
        
        except FileNotFoundError:
            print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {args.file}")
            sys.exit(1)
    
    # ç›®å½•ä¼°ç®—
    elif args.dir:
        results = estimator.estimate_directory(args.dir, args.pattern)
        
        if not results:
            print(f"æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶: {args.dir}/{args.pattern}")
            sys.exit(0)
        
        report = analyzer.generate_report(results)
        print(report)
        
        if args.compare:
            total_tokens = sum(e.get('tokens', 0) for e in results)
            print("\n" + "=" * 60)
            print("æ¨¡å‹æˆæœ¬å¯¹æ¯”:")
            print("=" * 60)
            comparisons = analyzer.compare_models(total_tokens, total_tokens // 2)
            for c in comparisons:
                print(f"  {c['model']:20} ${c['total_cost']:.4f}")
    
    # ä»»åŠ¡ä¼°ç®—
    elif args.task:
        result = estimator.estimate_task(args.task, args.complexity)
        cost = analyzer.calculate_cost(result['input_tokens'], result['output_tokens'])
        result['cost'] = cost
        
        if args.json:
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            print(f"ä»»åŠ¡: {result['task']}")
            print(f"ç±»å‹: {result['task_type']}")
            print(f"å¤æ‚åº¦: {result['complexity']} (x{result['multiplier']})")
            print(f"è¾“å…¥Token: ~{result['input_tokens']}")
            print(f"è¾“å‡ºToken: ~{result['output_tokens']}")
            print(f"æ€»è®¡: ~{result['total_tokens']} tokens")
            print(f"é¢„ä¼°æˆæœ¬: ${cost['total_cost']:.4f}")
    
    # æŠ¥å‘Š
    elif args.report:
        print("ç»Ÿè®¡æŠ¥å‘ŠåŠŸèƒ½å¼€å‘ä¸­...")
        print(f"ç»Ÿè®¡æ•°æ®ä¿å­˜åœ¨: {estimator.stats_path}")
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
