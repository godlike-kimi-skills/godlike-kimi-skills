# Firecrawl

**Web æ•°æ®æå–ä¸çˆ¬å–å¼•æ“** - ä¸“ä¸º AI åº”ç”¨è®¾è®¡çš„æ™ºèƒ½çˆ¬è™« API

ç½‘ç«™åœ°å›¾ç”Ÿæˆã€å…¨ç«™çˆ¬å–ã€ç»“æ„åŒ–æ•°æ®æå–ï¼Œå°†äº’è”ç½‘æ•°æ®è½¬åŒ–ä¸º AI å¯ç”¨æ ¼å¼ã€‚

---

## æ ¸å¿ƒèƒ½åŠ›

### ğŸ•·ï¸ çˆ¬å–æ¨¡å¼

| åŠŸèƒ½ | æè¿° | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **å•é¡µçˆ¬å–** (/scrape) | æå–å•ä¸ª URL | å¿«é€Ÿæå– |
| **å…¨ç«™çˆ¬å–** (/crawl) | é€’å½’çˆ¬å–æ•´ä¸ªç½‘ç«™ | ç«™ç‚¹å¤‡ä»½ |
| **ç½‘ç«™åœ°å›¾** (/map) | ç”Ÿæˆç«™ç‚¹ç»“æ„ | ç«™ç‚¹åˆ†æ |
| **æ‰¹é‡çˆ¬å–** (/batch) | æ‰¹é‡ URL å¤„ç† | å¤§è§„æ¨¡æå– |
| **æœç´¢çˆ¬å–** (/search) | æœç´¢+æå– | ç ”ç©¶é‡‡é›† |

### ğŸ“„ è¾“å‡ºæ ¼å¼

```
è¾“å‡ºé€‰é¡¹:
â”œâ”€â”€ Markdown (é»˜è®¤) - é€‚åˆ LLM å¤„ç†
â”œâ”€â”€ HTML - åŸå§‹ç½‘é¡µå†…å®¹
â”œâ”€â”€ Screenshot - é¡µé¢æˆªå›¾
â”œâ”€â”€ Links - æå–é“¾æ¥
â””â”€â”€ Structured Data - ç»“æ„åŒ–æ•°æ® (LLMæå–)
```

---

## ä½¿ç”¨æ–¹æ³•

### CLI å‘½ä»¤

```bash
# å•é¡µçˆ¬å–
firecrawl scrape "https://example.com/docs"

# å…¨ç«™çˆ¬å–
firecrawl crawl "https://example.com" --output sitemap.md

# ç”Ÿæˆç½‘ç«™åœ°å›¾
firecrawl map "https://example.com" --limit 1000

# æ‰¹é‡å¤„ç†
firecrawl batch --urls urls.txt --output ./data/

# å¸¦æˆªå›¾
firecrawl scrape "https://example.com" --formats markdown,screenshot

# æå–ç‰¹å®šå†…å®¹
firecrawl scrape "https://example.com" --extract "äº§å“åç§°,ä»·æ ¼,æè¿°"
```

### API è°ƒç”¨

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-...")

# å•é¡µçˆ¬å–
result = app.scrape_url("https://example.com", params={
    "formats": ["markdown", "html"]
})

# å…¨ç«™çˆ¬å–
crawl_result = app.crawl_url("https://example.com", params={
    "limit": 100,
    "scrapeOptions": {
        "formats": ["markdown"]
    }
}, wait_until_done=True)

# ç½‘ç«™åœ°å›¾
map_result = app.map_url("https://example.com", params={
    "search": "documentation",
    "limit": 1000
})
```

---

## é«˜çº§åŠŸèƒ½

### LLM æå–

```python
# ä½¿ç”¨ LLM æå–ç»“æ„åŒ–æ•°æ®
result = app.scrape_url("https://example.com/product/123", params={
    "formats": ["markdown"],
    "extract": {
        "schema": {
            "type": "object",
            "properties": {
                "product_name": {"type": "string"},
                "price": {"type": "number"},
                "description": {"type": "string"},
                "features": {"type": "array", "items": {"type": "string"}}
            }
        }
    }
})
```

### è‡ªå®šä¹‰è¡Œä¸º

```python
# ç­‰å¾…ç‰¹å®šå…ƒç´ 
result = app.scrape_url(url, params={
    "waitFor": 2000,  # ç­‰å¾… 2 ç§’
    "actions": [
        {"type": "click", "selector": "button.load-more"},
        {"type": "wait", "milliseconds": 1000}
    ]
})

# ç§»åŠ¨ç«¯çˆ¬å–
result = app.scrape_url(url, params={
    "mobile": True,
    "viewport": {"width": 375, "height": 667}
})
```

### æ‰¹é‡å¤„ç†

```python
# æ‰¹é‡ URL
urls = [
    "https://example.com/page1",
    "https://example.com/page2",
    ...
]

batch_result = app.batch_scrape_urls(urls, params={
    "formats": ["markdown"]
})
```

---

## æœ€ä½³å®è·µ

### çˆ¬å–ç­–ç•¥

```
1. å°Šé‡ robots.txt
2. æ§åˆ¶çˆ¬å–é¢‘ç‡ (å»ºè®® > 1s/è¯·æ±‚)
3. ä½¿ç”¨ sitemap é™åˆ¶èŒƒå›´
4. å¤„ç† JavaScript æ¸²æŸ“
5. é”™è¯¯é‡è¯•æœºåˆ¶
```

### é€Ÿç‡é™åˆ¶

```
å…è´¹è®¡åˆ’: 500 credits/month
â”œâ”€â”€ /scrape: 1 credit
â”œâ”€â”€ /crawl: 5 credits + 1/page
â””â”€â”€ /map: 1 credit

ä»˜è´¹è®¡åˆ’: ä» $16/month èµ·
```

### æ•°æ®æ¸…æ´—

```python
from firecrawl_connector import Cleaner

cleaner = Cleaner()

# æ¸…æ´— Markdown
clean_md = cleaner.clean(
    result["markdown"],
    remove_navigation=True,
    remove_ads=True,
    remove_footer=True
)

# æå–æ­£æ–‡
main_content = cleaner.extract_main_content(clean_md)
```

---

## ä¸ Tavily å¯¹æ¯”

| ç‰¹æ€§ | Firecrawl | Tavily |
|------|-----------|--------|
| **ä¸»è¦åŠŸèƒ½** | ç½‘ç«™çˆ¬å– | æœç´¢å¼•æ“ |
| **æ•°æ®èŒƒå›´** | æŒ‡å®šç½‘ç«™ | å…¨ç½‘æœç´¢ |
| **è¾“å‡ºæ ¼å¼** | Markdown/HTML | æ‘˜è¦+é“¾æ¥ |
| **ç»“æ„åŒ–** | LLM æå– | åŸºç¡€æå– |
| **å®æ—¶æ€§** | å–å†³äºç›®æ ‡ç«™ | å®æ—¶æœç´¢ |
| **ä½¿ç”¨åœºæ™¯** | ç«™ç‚¹åˆ†æã€æ–‡æ¡£æå– | ä¿¡æ¯æ£€ç´¢ |

### ç»„åˆä½¿ç”¨

```python
# 1. æœç´¢å‘ç°
tavily_results = tavily.search("best practices", max_results=10)
urls = [r["url"] for r in tavily_results["results"]]

# 2. æ·±åº¦çˆ¬å–
for url in urls:
    content = firecrawl.scrape(url)
    # å¤„ç†å†…å®¹...
```

---

## å‚è€ƒæ¥æº

- **Firecrawl**: https://firecrawl.dev
- **æ–‡æ¡£**: https://docs.firecrawl.dev
- **GitHub**: https://github.com/mendableai/firecrawl

---

## ç‰ˆæœ¬ä¿¡æ¯

- **Version**: 2.0.0 (2025 å¢å¼ºç‰ˆ)
- **Author**: KbotGenesis
- **API Version**: v1
- **Last Updated**: 2026-02-19
